---
# This configuration file has two sections.  The first is the Vagrant boxes 
# subdivided into ceph installation methods with related repositories and 
# packages.  The second is the various server configurations to simulate 
# different environments.
#
########################################
# Installation Section
########################################
#
# The vagrant box is a JeOS image and has no initial repositories.  Create new
# entries to use your own repositories or add repositories to the existing lists.
#
# Change the INSTALLATION to either 'ceph-deploy' or 'vsm' in the Vagrantfile.

########################################
# OpenSUSE Leap 42.2
########################################
opensuse/openSUSE-42.2-x86_64:
  salt:
    repos:
      'ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Leap_42.3/'
      '42.2': 'http://download.opensuse.org/distribution/leap/42.2/repo/oss/suse/'
    packages:
      all:
        - vim
        - salt-minion
      admin:
        - salt-master
        - ntp
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
      all:
        - systemctl disable SuSEfirewall2
        - systemctl stop SuSEfirewall2
        - systemctl restart salt-minion
        - systemctl enable salt-minion
          
########################################
# OpenSUSE Leap 42.3
########################################
opensuse/openSUSE-42.3-x86_64:
  salt:
    repos:
      'ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Leap_42.3/'
      '42.2': 'http://download.opensuse.org/ristribution/leap/42.3/repo/oss/suse/'
    packages:
      all:
        - vim
        - salt-minion
      admin:
        - salt-master
        - ntp
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
      all:
        - systemctl disable SuSEfirewall2
        - systemctl stop SuSEfirewall2
        - systemctl restart salt-minion
        - systemctl enable salt-minion

########################################
# SLE 12 SP2 SES4 migration
########################################
SLE12-SP2-migration:
  salt:
    register:
        - SUSEConnect -r 0000000000000
        - SUSEConnect -r 0000000000000 -p ses/4/x86_64
    packages:
      all:
        - vim
        - salt-minion
      admin:
        - salt-master
        - vim-data
        - ntp
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
      customized: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 20
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph 
        - chmod 1777 /tmp
        - zypper -n in deepsea
        - salt-run state.orch ceph.stage.0
      all:
        - systemctl disable SuSEfirewall2
        - systemctl stop SuSEfirewall2
        - systemctl restart salt-minion
        - systemctl enable salt-minion
        - zypper -n in kernel-default | true
        - zypper -n rm kernel-default-base | true
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master

          
########################################
# SLE 12 SP3 SES5
########################################
SLE12-SP3-patch:
  salt:
    register:
        - SUSEConnect -r 0000000000000
        - SUSEConnect -r 0000000000000 -p ses/4/x86_64
    packages:
      all:
        - vim
        - salt-minion
      admin:
        - salt-master
        - vim-data
        - ntp
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 20
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph 
        - chmod 1777 /tmp
      all:
        - systemctl disable SuSEfirewall2
        - systemctl stop SuSEfirewall2
        - systemctl restart salt-minion
        - systemctl enable salt-minion
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master

########################################
# SLE 12 SP3 SES5
########################################
SLE12-SP3:
  salt:
    repos:
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP3:/Update:/Products:/SES5/images/repo/SUSE-Enterprise-Storage-5-POOL-x86_64-Media1/'
      'Storage-update': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP3:/Update:/Products:/SES5:/Update/standard/'
      'SP3': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP3:/GA/images/repo/SLE-12-SP3-Server-POOL-x86_64-Media1/'
      'SP3-update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/'
    packages:
      all:
        - vim
        - salt-minion
      admin:
        - salt-master
        - vim-data
        - ntp
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph 
        - chmod 1777 /tmp
      all:
        - systemctl disable SuSEfirewall2
        - systemctl stop SuSEfirewall2
        - systemctl restart salt-minion
        - systemctl enable salt-minion


########################################
# SLE 12 SP2 SES4
########################################
SLE12-SP2:
  openattic:
    repos:
      'OpenAttic': 'http://download.opensuse.org/repositories/filesystems:/openATTIC/SLE_12_SP2/'
      'sle-sdk': 'http://download.suse.de/ibs/SUSE/Products/SLE-SDK/12-SP2/x86_64/product/'
      'sle-sdk update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SDK/12-SP2/x86_64/update/'
      'SLE 12 update': 'http://download.suse.de/ibs/SUSE/Products/SLE-SERVER/12-SP2/x86_64/product/'
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES5/standard/'
    packages:
      admin:
        #- monitoring-plugins-http
        #- monitoring-plugins-swap
        #- monitoring-plugins-ssh
        #- monitoring-plugins-ping
        #- monitoring-plugins-disk
        - pnp4nagios-0.6.25-8.3
        - openattic
        - openattic-gui
      all:
        - vim
        - vim-data
        - man
    files:
      admin: true
    commands:
      admin:
        - ln -s /etc/icinga /etc/nagios3
        - ln -s /etc/icinga/icinga.cfg /etc/icinga/nagios.cfg
        - ln -s /usr/sbin/icinga /usr/sbin/nagios
        - systemctl enable apache2
        - systemctl start apache2
        - oaconfig install --allow-broken-hostname
  salt:
    repos:
      'sle-sdk': 'http://download.suse.de/ibs/SUSE/Products/SLE-SDK/12-SP2/x86_64/product/'
      'sle-sdk update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SDK/12-SP2/x86_64/update/'
      'SLE 12 update': 'http://download.suse.de/ibs/SUSE/Products/SLE-SERVER/12-SP2/x86_64/product/'
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES4/images/repo/SUSE-Enterprise-Storage-4-POOL-x86_64-Media1/'
      'Salt': 'http://download.opensuse.org/repositories/systemsmanagement:/saltstack:/products/SLE_12_SP2/'
      'ses4updates': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update:/Products:/SES4:/Update/standard/'
      'sle12updates': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP2:/Update/standard/'
    packages:
      all:
        - vim
        - vim-data
        - python-setuptools
        - salt-minion
      admin:
        - salt-master
        - ntp
        - tree
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph 
        - chmod 1777 /tmp
      all:
        - systemctl restart salt-minion
        - systemctl enable salt-minion
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master

########################################
# SLE 12 SP3 SES5
########################################
SLE12-SP3-qa:
  salt:
    repos:
      'Storage': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP3:/Update:/Products:/SES5/images/repo/SUSE-Enterprise-Storage-5-POOL-x86_64-Media1/'
      'SP3-update': 'http://download.suse.de/ibs/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/'
      'SP3': 'http://download.suse.de/ibs/SUSE:/SLE-12-SP3:/GA/images/repo/SLE-12-SP3-Server-POOL-x86_64-Media1/'
    packages:
      all:
        - vim
        - salt-minion
      admin:
        - salt-master
        - vim-data
        - ntp
      client1:
        - multipath-tools
      client2:
        - multipath-tools
    files:
      all: false
      admin: true
    commands:
      admin:
        - systemctl start ntpd
        - systemctl enable ntpd
        - systemctl start salt-master
        - systemctl enable salt-master
        - sleep 10
        - salt-key -y -A  || exit 0
        - chown -R salt:salt /srv/pillar/ceph /srv/salt/ceph 
        - chmod 1777 /tmp
        - zypper -n in deepsea 
        - zypper -n in /root/deepsea-qa*
        - pushd /usr/lib/deepsea/qa; sh suites/basic/health-ok.sh; popd 
      all:
        - systemctl disable SuSEfirewall2
        - systemctl stop SuSEfirewall2
        - systemctl restart salt-minion
        - systemctl enable salt-minion
      calmari:
        - systemctl start salt-master
        - systemctl enable salt-master

########################################
# Configuration Section
########################################
#
# The default allows for reasonable exploration of Ceph installations and
# configurations.  Three monitors allow for a quorum and understanding failover,
# elections, etc.  Six data nodes with 30 osds and a pretend journal give
# enough flexibility to try replicas and erasure coding (e.g. k=3, m=2) while
# downing an osd or an entire node.
#
# This requires 5G RAM and ~32G disk space.  Initial vagrant up is ~28 minutes
# on a quad core.
dataonmon:
  description: "admin, 3 monitors, 5 data nodes with journals"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
  memory:
    admin: 2048
    mon1: 2048
    mon2: 2048
    mon3: 2048
    data1: 1536
    data2: 1536
    data3: 1536
    data4: 1536 
  disks:
    data1:
      hds: 5
      ssds: 1
    data2:
      hds: 5
      ssds: 1
    data3:
      hds: 5
      ssds: 1
    data4:
      hds: 5
      ssds: 1
default:
  description: "admin, 3 monitors, 5 data nodes with journals"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
      cluster: 172.16.2.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
      cluster: 172.16.2.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
    data5:
      management: 192.168.1.25
      public:  172.16.1.25
      cluster: 172.16.2.25
  memory:
    admin: 2048
    mon1: 2048
    mon2: 2048
    mon3: 2048
    data1: 1536
    data2: 1536
    data3: 1536
    data4: 1536 
    data5: 1536
  disks:
    data1:
      hds: 5
      ssds: 1
    data2:
      hds: 5
      ssds: 1
    data3:
      hds: 5
      ssds: 1
    data4:
      hds: 5
      ssds: 1
    data5:
      hds: 5
      ssds: 1
# The small configuration is a typical demo setup of a monitor and three data
# nodes with two osds each.  This can be ideal for varifying that Vagrant and
# libvirt are behaving.  Simpler installations can be verified and the ceph
# command line can be explored.
#
# This requires 2.5G RAM and ~7G disk space.  Initial vagrant up is ~13 minutes
# on a quad core.
tiny:
  description: "1 admin, 1mon, 1 data node"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
  memory:
    admin: 2048
    mon1: 2048
    data1: 2048
  disks:
    data1:
      hds: 5
      ssds: 1
small:
  description: "admin, 1 monitor, 3 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
  memory:
    admin: 1024
  disks:
    data1:
      hds: 30
    data2:
      hds: 30
    data3:
      hds: 30
iscsi:
  description: "admin, 2 clients, 3 monitors, 5 data nodes, 3 gateways, 2 mds and calamari"
  nodes:
    admin:
      management: 192.168.11.254
      public:  172.16.11.254
      cluster: 172.16.12.254
    client1:
      management: 192.168.11.5
      public:  172.16.11.5
      cluster: 172.16.12.5
    client2:
      management: 192.168.11.6
      public:  172.16.11.6
      cluster: 172.16.12.6
    calamari:
      management: 192.168.11.10
      public:  172.16.11.10
      cluster: 172.16.12.10
    mon1:
      management: 192.168.11.11
      public:  172.16.11.11
      cluster: 172.16.12.11
    mon2:
      management: 192.168.11.12
      public:  172.16.11.12
      cluster: 172.16.12.12
    mon3:
      management: 192.168.11.13
      public:  172.16.11.13
      cluster: 172.16.12.13
    mds1:
      management: 192.168.11.14
      public:  172.16.11.14
      cluster: 172.16.12.14
    mds2:
      management: 192.168.11.15
      public:  172.16.11.15
      cluster: 172.16.12.15
    igw1:
      management: 192.168.11.16
      public:  172.16.11.16
      cluster: 172.16.12.16
    igw2:
      management: 192.168.11.17
      public:  172.16.11.17
      cluster: 172.16.12.17
    igw3:
      management: 192.168.11.18
      public:  172.16.11.18
      cluster: 172.16.12.18
    data1:
      management: 192.168.11.21
      public:  172.16.11.21
      cluster: 172.16.12.21
    data2:
      management: 192.168.11.22
      public:  172.16.11.22
      cluster: 172.16.12.22
    data3:
      management: 192.168.11.23
      public:  172.16.11.23
      cluster: 172.16.12.23
    data4:
      management: 192.168.11.24
      public:  172.16.11.24
      cluster: 172.16.12.24
    data5:
      management: 192.168.11.25
      public:  172.16.11.25
      cluster: 172.16.12.25
  cpu:
    admin: 4
  memory:
    admin: 3072
    calamari: 2048
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
fifteen:
  description: "admin, 2 clients, 3 monitors, 6 data nodes, 3 gateways"
  nodes:
    admin:
      management: 192.168.21.254
      public:  172.16.21.254
      cluster: 172.16.22.254
    client1:
      management: 192.168.21.5
      public:  172.16.21.5
      cluster: 172.16.22.5
    client2:
      management: 192.168.21.6
      public:  172.16.21.6
      cluster: 172.16.22.6
    mon1:
      management: 192.168.21.11
      public:  172.16.21.11
      cluster: 172.16.22.11
    mon2:
      management: 192.168.21.12
      public:  172.16.21.12
      cluster: 172.16.22.12
    mon3:
      management: 192.168.21.13
      public:  172.16.21.13
      cluster: 172.16.22.13
    igw1:
      management: 192.168.21.16
      public:  172.16.21.16
      cluster: 172.16.22.16
    igw2:
      management: 192.168.21.17
      public:  172.16.21.17
      cluster: 172.16.22.17
    igw3:
      management: 192.168.21.18
      public:  172.16.21.18
      cluster: 172.16.22.18
    data1:
      management: 192.168.21.21
      public:  172.16.21.21
      cluster: 172.16.22.21
    data2:
      management: 192.168.21.22
      public:  172.16.21.22
      cluster: 172.16.22.22
    data3:
      management: 192.168.21.23
      public:  172.16.21.23
      cluster: 172.16.22.23
    data4:
      management: 192.168.21.24
      public:  172.16.21.24
      cluster: 172.16.22.24
    data5:
      management: 192.168.21.25
      public:  172.16.21.25
      cluster: 172.16.22.25
    data6:
      management: 192.168.21.26
      public:  172.16.21.26
      cluster: 172.16.22.26
  cpu:
    admin: 4
  memory:
    admin: 3072
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
    data6:
      hds: 2
sixteen:
  description: "admin, 2 clients, 3 monitors, 5 data nodes, 3 gateways, 2 mds"
  nodes:
    admin:
      management: 192.168.31.254
      public:  172.16.31.254
      cluster: 172.16.32.254
    client1:
      management: 192.168.31.5
      public:  172.16.31.5
      cluster: 172.16.32.5
    client2:
      management: 192.168.31.6
      public:  172.16.31.6
      cluster: 172.16.32.6
    mon1:
      management: 192.168.31.11
      public:  172.16.31.11
      cluster: 172.16.32.11
    mon2:
      management: 192.168.31.12
      public:  172.16.31.12
      cluster: 172.16.32.12
    mon3:
      management: 192.168.31.13
      public:  172.16.31.13
      cluster: 172.16.32.13
    mds1:
      management: 192.168.31.14
      public:  172.16.31.14
      cluster: 172.16.32.14
    mds2:
      management: 192.168.31.15
      public:  172.16.31.15
      cluster: 172.16.32.15
    igw1:
      management: 192.168.31.16
      public:  172.16.31.16
      cluster: 172.16.32.16
    igw2:
      management: 192.168.31.17
      public:  172.16.31.17
      cluster: 172.16.32.17
    igw3:
      management: 192.168.31.18
      public:  172.16.31.18
      cluster: 172.16.32.18
    data1:
      management: 192.168.31.21
      public:  172.16.31.21
      cluster: 172.16.32.21
    data2:
      management: 192.168.31.22
      public:  172.16.31.22
      cluster: 172.16.32.22
    data3:
      management: 192.168.31.23
      public:  172.16.31.23
      cluster: 172.16.32.23
    data4:
      management: 192.168.31.24
      public:  172.16.31.24
      cluster: 172.16.32.24
    data5:
      management: 192.168.31.25
      public:  172.16.31.25
      cluster: 172.16.32.25
  cpu:
    admin: 4
  memory:
    admin: 3072
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
eighteen:
  description: "master, 2 clients, 3 monitors, 6 data nodes, 3 igw, 3 rgw"
  nodes:
    admin:
      management: 192.168.21.254
      public:  172.16.21.254
      cluster: 172.16.22.254
    client1:
      management: 192.168.21.5
      public:  172.16.21.5
      cluster: 172.16.22.5
    client2:
      management: 192.168.21.6
      public:  172.16.21.6
      cluster: 172.16.22.6
    rgw1:
      management: 192.168.21.8
      public:  172.16.21.8
      cluster: 172.16.22.8
    rgw2:
      management: 192.168.21.9
      public:  172.16.21.9
      cluster: 172.16.22.9
    rgw3:
      management: 192.168.21.10
      public:  172.16.21.10
      cluster: 172.16.22.10
    mon1:
      management: 192.168.21.11
      public:  172.16.21.11
      cluster: 172.16.22.11
    mon2:
      management: 192.168.21.12
      public:  172.16.21.12
      cluster: 172.16.22.12
    mon3:
      management: 192.168.21.13
      public:  172.16.21.13
      cluster: 172.16.22.13
    igw1:
      management: 192.168.21.16
      public:  172.16.21.16
      cluster: 172.16.22.16
    igw2:
      management: 192.168.21.17
      public:  172.16.21.17
      cluster: 172.16.22.17
    igw3:
      management: 192.168.21.18
      public:  172.16.21.18
      cluster: 172.16.22.18
    data1:
      management: 192.168.21.21
      public:  172.16.21.21
      cluster: 172.16.22.21
    data2:
      management: 192.168.21.22
      public:  172.16.21.22
      cluster: 172.16.22.22
    data3:
      management: 192.168.21.23
      public:  172.16.21.23
      cluster: 172.16.22.23
    data4:
      management: 192.168.21.24
      public:  172.16.21.24
      cluster: 172.16.22.24
    data5:
      management: 192.168.21.25
      public:  172.16.21.25
      cluster: 172.16.22.25
    data6:
      management: 192.168.21.26
      public:  172.16.21.26
      cluster: 172.16.22.26
  cpu:
    admin: 4
  memory:
    admin: 3072
    rgw1: 768
    rgw2: 768
    rgw3: 768
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
    data1: 1024
    data2: 1024
    data3: 1024
    data4: 1024
    data5: 1024
    data6: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
    data4:
      hds: 2
    data5:
      hds: 2
    data6:
      hds: 2
# The economical configuration represents a cold storage configuration that has
# no SSD journals.  Additionally, the monitors and admin node do not have an
# interface on the cluster network to save money on one less 10G interface.
#
# This requires 4G RAM and ~17G disk space.  Initial vagrant up is ~17 minutes
# on a quad core.
economical:
  description: "admin, 3 monitors, 4 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
  memory:
    admin: 1024
  disks:
    data1:
      hds: 4
    data2:
      hds: 4
    data3:
      hds: 4
    data4:
      hds: 4

